
Docker combined Ollama and WebUI with GPU suport
# docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
docker run -d -p 3000:8080 --device /dev/kfd --device /dev/dri --v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama

### works !!!
# ollama image to use rocm
docker run -d --restart always --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm

# open-webui to use ollama:rocm
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
###

# used for accessing gpu via rocm
docker run --device /dev/kfd --device /dev/dri 


Install Ollama
>
curl -fsSL https://ollama.com/install.sh | sh

download a model
https://ollama.com/library/llama3
>
ollama pull llama3

run a model
>
ollama run llama3

webui
>
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main


monitor gpu
>
sudo apt install radeontop
sudo radeontop

download another model
ollama pull codegemma

multimodal model
ollama pull llava
 
 -choose @llava model when asking about pictures


Continue - CodeLlama
VS Code plugin 
https://www.continue.dev/
Open-source autopilot - build your own AI software development system

Stable Diffusion Install
Prereqs
Pyenv

#Install Pyenv prereqs
sudo apt install -y make build-essential libssl-dev zlib1g-dev \
libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \
libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev git

#Install Pyenv

curl https://pyenv.run | bash

#Install Python 3.10

pyenv install 3.10

#Make it global

pyenv global 3.10

 
Install Stable Diffusion

wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh

# Make it executable

chmod +x webui.sh

#Run it

./webui.sh --listen --api


stable diffusion on AMD Radeon
https://www.youtube.com/watch?v=d_CgaHyA_n4&t=79s

install ROCm kernel drivers
https://rocm.docs.amd.com/projects/install-on-linux/en/latest/how-to/native-install/ubuntu.html

install via AMDGPU installer
https://rocm.docs.amd.com/projects/install-on-linux/en/latest/how-to/amdgpu-install.html

troubleshooting
https://github.com/ROCm/ROCm/issues/3701

ROCm post install
https://rocm.docs.amd.com/projects/install-on-linux/en/latest/how-to/native-install/post-install.html

??? is this step necessary ???
run ROCm in docker
https://github.com/ROCm/ROCm-docker/blob/master/quick-start.md
???

rocm/pytorch
https://hub.docker.com/r/rocm/pytorch
add alias to .zshrc
drun rocm/pytorchâ€‹

Installing PyTorch for ROCm
https://rocm.docs.amd.com/projects/install-on-linux/en/latest/how-to/3rd-party/pytorch-install.html

pytorch installation info - 
>
pip show torch

-- docker pytorch setup, didn't work for me
https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs#running-inside-docker

pytorch installation
>
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0

Each time you open a new terminal it will need to be re-activated if you want to use PyTorch or any other PIP packages.
>
. venv/bin/activate

6700xt instructions
https://phazertech.com/tutorials/rocm.html
video for above instructions
https://www.youtube.com/watch?v=hBMvM9eQhPs


dreamshaper models
https://civitai.com/models/4384/dreamshaper

Run stable-diffusion
./webui.sh --api
